"""
Hybrid RAG Pipeline - Haystack + LangChain Fallback

M·ª•c ƒë√≠ch:
- T·∫°o RAG pipeline s·ª≠ d·ª•ng Haystack framework (∆∞u ti√™n)
- Fallback sang LangChain n·∫øu Haystack kh√¥ng kh·∫£ d·ª•ng
- X·ª≠ l√Ω document ingestion v√† query processing
- Qu·∫£n l√Ω vector store v√† embedding

C√¥ng ngh·ªá s·ª≠ d·ª•ng:
- Haystack: RAG framework ch√≠nh (n·∫øu kh·∫£ d·ª•ng)
- LangChain: Fallback RAG framework
- OpenAI: LLM cho text generation
- Qdrant: Vector store cho persistence

Ki·∫øn tr√∫c:
1. Document Processing: Chunking v√† embedding
2. Vector Storage: Qdrant Cloud
3. Retrieval: Semantic search v·ªõi top-k results
4. Generation: LLM response v·ªõi context
"""

from typing import List, Dict, Any, Optional
import os
from datetime import datetime
import uuid
import logging

# Configure logging
logger = logging.getLogger(__name__)

# Try Haystack first
HAYSTACK_AVAILABLE = False
try:
    # Create a clean environment for Haystack
    import os
    import sys
    
    # Temporarily remove pandas from sys.modules
    original_modules = {}
    problematic_modules = ['pandas', 'numpy', 'pandas.core.frame']
    
    for module_name in problematic_modules:
        if module_name in sys.modules:
            original_modules[module_name] = sys.modules[module_name]
            del sys.modules[module_name]
    
    # Set environment variables for Haystack
    os.environ['PYDANTIC_ARBITRARY_TYPES_ALLOWED'] = 'true'
    os.environ['PYDANTIC_IGNORE_UNKNOWN'] = 'true'
    
    # Import Haystack components
    from haystack import Pipeline
    from haystack.document_stores import InMemoryDocumentStore
    from haystack.nodes import EmbeddingRetriever, PromptNode, PromptTemplate
    
    # Restore original modules
    for module_name, module in original_modules.items():
        sys.modules[module_name] = module
    
    HAYSTACK_AVAILABLE = True
    logger.info("‚úÖ Haystack enabled successfully")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Haystack not available: {e}")
    # Restore original modules if failed
    if 'original_modules' in locals():
        for module_name, module in original_modules.items():
            sys.modules[module_name] = module

# Try LangChain as fallback
LANGCHAIN_AVAILABLE = False
try:
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain_community.vectorstores import FAISS
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.prompts import PromptTemplate as LangChainPromptTemplate
    from langchain.chains import LLMChain
    
    LANGCHAIN_AVAILABLE = True
    logger.info("‚úÖ LangChain enabled as fallback")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è LangChain not available: {e}")

if not HAYSTACK_AVAILABLE and not LANGCHAIN_AVAILABLE:
    logger.error("‚ùå Neither Haystack nor LangChain available!")

from config import config


class HybridRAGPipeline:
    """
    Hybrid RAG Pipeline
    
    Ch·ª©c nƒÉng ch√≠nh:
    1. Kh·ªüi t·∫°o pipeline v·ªõi Haystack framework (∆∞u ti√™n)
    2. Fallback sang LangChain n·∫øu Haystack kh√¥ng kh·∫£ d·ª•ng
    3. X·ª≠ l√Ω document ingestion
    4. Th·ª±c hi·ªán query processing
    5. Qu·∫£n l√Ω vector store
    """

    def __init__(self):
        """
        Kh·ªüi t·∫°o Hybrid RAG Pipeline
        - Ki·ªÉm tra availability c·ªßa Haystack v√† LangChain
        - Kh·ªüi t·∫°o pipeline ph√π h·ª£p
        """
        if HAYSTACK_AVAILABLE:
            self._init_haystack()
            self.active_pipeline = "Haystack"
            logger.info("üéØ Using Haystack as primary RAG pipeline")
        elif LANGCHAIN_AVAILABLE:
            self._init_langchain()
            self.active_pipeline = "LangChain"
            logger.info("üîÑ Using LangChain as fallback RAG pipeline")
        else:
            raise ImportError("Neither Haystack nor LangChain available")

    def _init_haystack(self):
        """
        Kh·ªüi t·∫°o Haystack pipeline
        
        Components:
        - Document Store: InMemoryDocumentStore v·ªõi cosine similarity
        - Retriever: EmbeddingRetriever v·ªõi OpenAI embeddings
        - Prompt Template: Template t√πy ch·ªânh cho ti·∫øng Vi·ªát
        - LLM: PromptNode v·ªõi OpenAI model
        """
        try:
            # Document store
            self.document_store = InMemoryDocumentStore(
                embedding_dim=config.models.embedding_dimension, similarity="cosine"
            )

            # Retriever v·ªõi top_k cao h∆°n ƒë·ªÉ c√≥ nhi·ªÅu context
            self.retriever = EmbeddingRetriever(
                document_store=self.document_store,
                embedding_model=config.models.embedding_model,
                model_format="openai",
                api_key=config.openai_api_key,
                top_k=min(config.processing.top_k * 2, 10),  # L·∫•y nhi·ªÅu documents h∆°n
            )

            # Prompt template
            self.prompt_template = PromptTemplate(
                prompt="""
                B·∫°n l√† tr·ª£ l√Ω d·ªØ li·ªáu, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n NG·ªÆ C·∫¢NH ƒë∆∞·ª£c cung c·∫•p.
                Y√äU C·∫¶U: 
                - Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh
                - Kh√¥ng t·ª± th√™m th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh
                - N·∫øu c√≥ b·∫£ng: hi·ªÉn th·ªã b·∫£ng ch√≠nh x√°c
                - N·∫øu c√≥ bullet points: li·ªát k√™ ch√≠nh x√°c

                H∆∞·ªõng d·∫´n tr·∫£ l·ªùi (b·∫±ng ti·∫øng Vi·ªát):
                - N·∫øu c√≥ b·∫£ng: xu·∫•t l·∫°i b·∫£ng Markdown ƒë·∫ßy ƒë·ªß t·ª´ d·ªØ li·ªáu trong ng·ªØ c·∫£nh
                - N·∫øu c√≥ bullet points: li·ªát k√™ ch√≠nh x√°c theo ng·ªØ c·∫£nh
                - N·∫øu c√≥ s·ªë li·ªáu: tr√≠ch ƒë√∫ng s·ªë, k√®m ƒë∆°n v·ªã
                - N·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ th√¥ng tin li√™n quan: tr·∫£ l·ªùi: 'Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu ƒë√£ cung c·∫•p.'

                [Ng·ªØ c·∫£nh]:
                {join(documents)}

                [C√¢u h·ªèi]: {query}

                Tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
                """,
                output_parser=None,
            )

            # LLM
            self.prompt_node = PromptNode(
                model_name_or_path=config.models.llm_model,
                api_key=config.openai_api_key,
                default_prompt_template=self.prompt_template,
                model_kwargs={"temperature": 0.2, "max_tokens": 1200},
            )

            # Build pipeline
            self.pipeline = self._build_haystack_pipeline()
            logger.info("‚úÖ Haystack pipeline initialized successfully")

        except Exception as e:
            logger.error(f"‚ùå Haystack initialization failed: {e}")
            raise e

    def _init_langchain(self):
        """
        Kh·ªüi t·∫°o LangChain pipeline
        
        Components:
        - Document Store: FAISS v·ªõi OpenAI embeddings
        - Retriever: FAISS similarity search
        - Prompt Template: LangChain PromptTemplate
        - LLM: LangChain LLMChain
        """
        try:
            # Embeddings
            self.embeddings = OpenAIEmbeddings(
                model=config.models.embedding_model,
                api_key=config.openai_api_key,
            )

            # Document store - start with empty texts
            self.document_store = FAISS.from_texts(
                texts=[""],
                embedding=self.embeddings,
            )

            # Text splitter
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=config.processing.chunk_size,
                chunk_overlap=config.processing.chunk_overlap,
            )

            # Prompt template
            self.prompt_template = LangChainPromptTemplate(
                input_variables=["context", "query"],
                template="""
                B·∫°n l√† tr·ª£ l√Ω d·ªØ li·ªáu, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n NG·ªÆ C·∫¢NH ƒë∆∞·ª£c cung c·∫•p.
                Y√äU C·∫¶U: 
                - Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh
                - Kh√¥ng t·ª± th√™m th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh
                - N·∫øu c√≥ b·∫£ng: hi·ªÉn th·ªã b·∫£ng ch√≠nh x√°c
                - N·∫øu c√≥ bullet points: li·ªát k√™ ch√≠nh x√°c

                H∆∞·ªõng d·∫´n tr·∫£ l·ªùi (b·∫±ng ti·∫øng Vi·ªát):
                - N·∫øu c√≥ b·∫£ng: xu·∫•t l·∫°i b·∫£ng Markdown ƒë·∫ßy ƒë·ªß t·ª´ d·ªØ li·ªáu trong ng·ªØ c·∫£nh
                - N·∫øu c√≥ bullet points: li·ªát k√™ ch√≠nh x√°c theo ng·ªØ c·∫£nh
                - N·∫øu c√≥ s·ªë li·ªáu: tr√≠ch ƒë√∫ng s·ªë, k√®m ƒë∆°n v·ªã
                - N·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ th√¥ng tin li√™n quan: tr·∫£ l·ªùi: 'Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu ƒë√£ cung c·∫•p.'

                [Ng·ªØ c·∫£nh]:
                {context}

                [C√¢u h·ªèi]: {query}

                Tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
                """,
            )

            # LLM
            self.llm = ChatOpenAI(
                model=config.models.llm_model,
                api_key=config.openai_api_key,
                temperature=0.2,
                max_tokens=1200,
            )

            # Chain
            self.chain = LLMChain(
                llm=self.llm,
                prompt=self.prompt_template,
            )

            logger.info("‚úÖ LangChain pipeline initialized successfully")

        except Exception as e:
            logger.error(f"‚ùå LangChain initialization failed: {e}")
            raise e

    def _build_haystack_pipeline(self):
        """Build Haystack RAG pipeline"""
        pipeline = Pipeline()
        pipeline.add_node(component=self.retriever, name="Retriever", inputs=["Query"])
        pipeline.add_node(
            component=self.prompt_node, name="PromptNode", inputs=["Retriever"]
        )
        return pipeline

    def add_documents(self, documents):
        """Add documents to the pipeline"""
        try:
            if self.active_pipeline == "Haystack":
                # Add to Haystack document store
                self.document_store.write_documents(documents)
                self.retriever.update_embeddings(documents)
            else:  # LangChain
                # Process documents for LangChain
                texts = []
                metadatas = []
                for doc in documents:
                    # Split text into chunks
                    chunks = self.text_splitter.split_text(doc.page_content)
                    for chunk in chunks:
                        texts.append(chunk)
                        metadatas.append({
                            "source_name": doc.metadata.get("source_name", "Unknown"),
                            "page": doc.metadata.get("page", 0)
                        })
                
                # Add to FAISS
                self.document_store.add_texts(texts=texts, metadatas=metadatas)
            
            logger.info(f"‚úÖ Added {len(documents)} documents to {self.active_pipeline} pipeline")
        except Exception as e:
            logger.error(f"‚ùå Error adding documents: {e}")
            raise e

    def query(self, query: str) -> Dict[str, Any]:
        """Query the pipeline"""
        try:
            if self.active_pipeline == "Haystack":
                # Use Haystack pipeline
                result = self.pipeline.run(query=query)
                return {
                    "answer": (
                        result["answers"][0].answer
                        if result["answers"]
                        else "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi."
                    ),
                    "documents": result["documents"],
                    "sources": [
                        doc.meta.get("source_name", "Unknown")
                        for doc in result["documents"]
                    ],
                    "pipeline": self.active_pipeline,
                }
            else:  # LangChain
                # Use LangChain pipeline
                # Search for relevant documents
                docs = self.document_store.similarity_search(
                    query, k=config.processing.top_k
                )
                
                if not docs:
                    return {
                        "answer": "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu ƒë√£ cung c·∫•p.",
                        "documents": [],
                        "sources": [],
                        "pipeline": self.active_pipeline,
                    }
                
                # Combine context
                context = "\n\n".join([doc.page_content for doc in docs])
                
                # Generate answer
                result = self.chain.run(context=context, query=query)
                
                return {
                    "answer": result,
                    "documents": docs,
                    "sources": [
                        doc.metadata.get("source_name", "Unknown")
                        for doc in docs
                    ],
                    "pipeline": self.active_pipeline,
                }
        except Exception as e:
            logger.error(f"‚ùå Error in RAG pipeline: {e}")
            return {
                "answer": "C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi.",
                "documents": [],
                "sources": [],
                "pipeline": "Error",
            }

    def get_document_count(self) -> int:
        """Get number of documents in store"""
        try:
            if self.active_pipeline == "Haystack":
                return self.document_store.get_document_count()
            else:  # LangChain
                return len(self.document_store.index_to_docstore_id)
        except Exception as e:
            logger.error(f"‚ùå Error getting document count: {e}")
            return 0

    def get_pipeline_info(self) -> Dict[str, Any]:
        """Get pipeline information"""
        return {
            "document_count": self.get_document_count(),
            "pipeline_type": "Hybrid RAG Pipeline",
            "active_pipeline": self.active_pipeline,
            "components": [
                "OpenAIEmbedder",
                "InMemoryEmbeddingRetriever", 
                "OpenAIGenerator",
            ],
        }


# Global pipeline instance
rag_pipeline = None

def get_rag_pipeline():
    global rag_pipeline
    if rag_pipeline is None:
        try:
            rag_pipeline = HybridRAGPipeline()
            logger.info(
                f"‚úÖ Hybrid RAG Pipeline initialized successfully"
            )
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Hybrid RAG Pipeline: {e}")
            rag_pipeline = None
    return rag_pipeline
