# -*- coding: utf-8 -*-
"""
ğŸ¤– Hybrid RAG Chatbot with Image Support
========================================

ğŸ“‹ MÃ´ táº£: á»¨ng dá»¥ng RAG chatbot hybrid vá»›i Haystack + LangChain fallback
ğŸ¯ Má»¥c Ä‘Ã­ch: Xá»­ lÃ½ tÃ i liá»‡u Ä‘a Ä‘á»‹nh dáº¡ng vÃ  chat tÆ°Æ¡ng tÃ¡c
ğŸ”§ Architecture: Hybrid approach vá»›i auto-fallback cho reliability cao

ğŸ“Š Luá»“ng Ä‘i chÃ­nh:
1. ğŸš€ Initialization: Load services, NLTK data, chat history
2. ğŸ“ File Management: Upload, process, auto-reload documents
3. ğŸ’¬ Chat Interface: Process queries vá»›i Hybrid RAG pipeline
4. ğŸ–¼ï¸ Image Support: Extract vÃ  display relevant images
5. ğŸ’¾ Persistence: Save chat history vÃ  session state

ğŸ”§ Key Components:
- @st.cache_resource: Cache services Ä‘á»ƒ performance
- @lru_cache: Cache chat history Ä‘á»ƒ trÃ¡nh re-reading
- Session State: Quáº£n lÃ½ UI state vÃ  user data
- Error Handling: Graceful degradation vá»›i fallback
- File Processing: Multi-format support vá»›i DocumentService

ğŸ“ˆ Performance Optimizations:
- Caching strategies cho services vÃ  data
- Lazy loading cho documents
- Pagination cho file lists
- Smart search vÃ  filtering
- Memory efficient processing

ğŸ”„ Hybrid Pipeline Logic:
- Primary: Haystack Core (fast, feature-rich)
- Fallback: LangChain (reliable, simple)
- Auto-switch: Exception-based fallback
- Unified: Consistent result format

ğŸ“ File Structure:
- uploads/: Stored uploaded files
- image_database/: Extracted images
- chat_history.json: Persistent chat data
- Session state: Runtime data management

ğŸ¨ UI Features:
- Smart file list vá»›i search vÃ  pagination
- File type icons vÃ  truncation
- Real-time chat interface
- Image display vá»›i download
- Progress indicators vÃ  error handling
"""

import streamlit as st
import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Any
from functools import lru_cache

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
from dotenv import load_dotenv

load_dotenv()

# Import all services at top level for better performance
from app_factory import initialize_app, get_app_factory
from services.document_service import DocumentService

# Download NLTK data once at startup
try:
    import nltk
    import ssl

    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# Global constants
CHAT_HISTORY_FILE = "chat_history.json"
UPLOADS_DIR = "uploads"
MAX_FILENAME_LENGTH = 25


def download_nltk_data():
    """
    ğŸ“š Download required NLTK data for markdown processing

    ğŸ”„ Luá»“ng Ä‘i:
    1. Download punkt: Sentence tokenization
    2. Download averaged_perceptron_tagger: POS tagging
    3. Download maxent_ne_chunker: Named entity recognition
    4. Download words: Word list
    5. Download stopwords: Stop words removal

    ğŸ“Š Má»¥c Ä‘Ã­ch: Há»— trá»£ xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh
    """
    try:
        nltk.download("punkt", quiet=True)
        nltk.download("averaged_perceptron_tagger", quiet=True)
        nltk.download("maxent_ne_chunker", quiet=True)
        nltk.download("words", quiet=True)
        nltk.download("stopwords", quiet=True)
        return True
    except Exception as e:
        st.warning(f"âš ï¸ Warning: Could not download NLTK data: {e}")
        return False


@lru_cache(maxsize=1)
def load_chat_history() -> List[Dict[str, Any]]:
    """
    ğŸ“š Load chat history tá»« JSON file vá»›i caching

    ğŸ”„ Luá»“ng Ä‘i:
    1. Check file tá»“n táº¡i
    2. Load JSON vá»›i UTF-8 encoding
    3. Cache result Ä‘á»ƒ trÃ¡nh re-reading
    4. Return empty list náº¿u file khÃ´ng tá»“n táº¡i

    ğŸ“Š Performance: @lru_cache(maxsize=1) Ä‘á»ƒ cache result
    ğŸ’¾ Persistence: UTF-8 encoding cho Vietnamese text
    """
    try:
        if os.path.exists(CHAT_HISTORY_FILE):
            with open(CHAT_HISTORY_FILE, "r", encoding="utf-8") as f:
                history = json.load(f)
                return history
        else:
            return []
    except Exception as e:
        logger.error(f"âŒ Error loading chat history: {e}")
        return []


def save_chat_history(chat_history: List[Dict[str, Any]]) -> None:
    """
    ğŸ’¾ Save chat history vÃ o JSON file

    ğŸ”„ Luá»“ng Ä‘i:
    1. Write JSON vá»›i UTF-8 encoding
    2. Clear cache Ä‘á»ƒ force reload
    3. Log success/failure

    ğŸ“Š Format: JSON vá»›i ensure_ascii=False cho Vietnamese
    ğŸ”„ Cache: Clear cache sau khi save Ä‘á»ƒ consistency
    """
    try:
        with open(CHAT_HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(chat_history, f, ensure_ascii=False, indent=2)
        # Clear cache to force reload
        load_chat_history.cache_clear()
    except Exception as e:
        logger.error(f"âŒ Error saving chat history: {e}")


def restore_chat_from_weaviate(rag_pipeline) -> int:
    """Load chats from Weaviate 'Chats' collection into session and persist to file."""
    try:
        if not hasattr(rag_pipeline, "document_store"):
            return 0
        store = rag_pipeline.document_store
        if not hasattr(store, "client"):
            return 0
        coll = store.client.collections.get("Chats")
        rows: List[Dict[str, Any]] = []
        for obj in coll.iterator():
            props = getattr(obj, "properties", {}) or {}
            rows.append(
                {
                    "question": props.get("question", ""),
                    "answer": props.get("answer", ""),
                    "sources": (
                        props.get("sources", "").split(", ") if props.get("sources") else []
                    ),
                    "timestamp": props.get("timestamp", ""),
                }
            )
        rows.sort(key=lambda r: r.get("timestamp", ""))
        st.session_state.chat_history = rows
        save_chat_history(rows)
        return len(rows)
    except Exception as e:
        logger.error(f"âŒ Restore chats from Weaviate failed: {e}")
        return 0


def clear_weaviate_chats(rag_pipeline) -> bool:
    """Delete Chats collection on Weaviate and recreate empty one."""
    try:
        if not hasattr(rag_pipeline, "document_store"):
            return False
        store = rag_pipeline.document_store
        if not hasattr(store, "client"):
            return False
        try:
            store.client.collections.delete("Chats")
        except Exception:
            pass
        if hasattr(store, "_ensure_collection"):
            store._ensure_collection()
        return True
    except Exception as e:
        logger.error(f"âŒ Clear Weaviate chats failed: {e}")
        return False


@st.cache_data(ttl=3600)  # Cache for 1 hour
def get_uploaded_files() -> List[str]:
    """
    ğŸ“ Get list of uploaded files vá»›i caching

    ğŸ”„ Luá»“ng Ä‘i:
    1. Check uploads directory tá»“n táº¡i
    2. List all files trong directory
    3. Filter chá»‰ files (khÃ´ng folders)
    4. Cache result cho 1 hour

    ğŸ“Š Performance: @st.cache_data(ttl=3600) Ä‘á»ƒ cache 1 giá»
    ğŸ” Purpose: TrÃ¡nh re-scanning directory má»—i láº§n
    """
    if not os.path.exists(UPLOADS_DIR):
        return []

    files = [f for f in os.listdir(UPLOADS_DIR) if os.path.isfile(os.path.join(UPLOADS_DIR, f))]
    return files


def auto_reload_documents(rag_pipeline, image_database):
    """Auto-reload documents from uploads folder with minimal logging"""
    # DISABLED: This function is disabled to prevent infinite loops
    # Documents are now loaded manually in main()
    pass


@st.cache_resource
def initialize_services():
    """
    ğŸ”§ Initialize services vá»›i caching cho performance tá»‘i Æ°u

    ğŸ”„ Luá»“ng Ä‘i:
    1. Gá»i initialize_app() tá»« app_factory
    2. Láº¥y AppFactory instance
    3. Cache result Ä‘á»ƒ trÃ¡nh re-initialization
    4. Return factory Ä‘á»ƒ access services

    ğŸ“Š Performance: @st.cache_resource Ä‘á»ƒ cache services
    ğŸ”„ Purpose: TrÃ¡nh re-initializing services má»—i láº§n
    """
    initialize_app()
    app_factory = get_app_factory()
    return app_factory


def debug_session_state():
    """Debug session state with minimal output"""
    # Only log if there are issues
    if not st.session_state.get("processed_files"):
        logger.info("ğŸ“„ No files in session state")

    # Only show files in uploads dir if needed for debugging
    if os.path.exists(UPLOADS_DIR):
        files = [f for f in os.listdir(UPLOADS_DIR) if os.path.isfile(os.path.join(UPLOADS_DIR, f))]
        if not files:
            logger.info("ğŸ“ Uploads directory is empty")


def main():
    """
    ğŸš€ Main application function - Khá»Ÿi táº¡o vÃ  quáº£n lÃ½ toÃ n bá»™ á»©ng dá»¥ng

    ğŸ”„ Luá»“ng Ä‘i:
    1. Initialize services vá»›i caching (@st.cache_resource)
    2. Download NLTK data má»™t láº§n duy nháº¥t
    3. Load chat history tá»« file JSON
    4. Auto-reload documents tá»« uploads folder
    5. Render UI components:
       - Sidebar vá»›i file management
       - Main content vá»›i chat interface
       - File upload section
       - Chat history display

    ğŸ“Š Session State Management:
    - chat_history: Chat messages
    - processed_files: Uploaded files
    - auto_reloaded: Auto-reload flag
    - force_show_upload: Show upload area
    - show_all_files: File list pagination
    - last_displayed_images: Last shown images
    - file_search: File search term

    ğŸ¨ UI Components:
    - Smart file list vá»›i search vÃ  pagination
    - File type icons vÃ  truncation
    - Real-time chat interface
    - Image display vá»›i download
    - Progress indicators vÃ  error handling
    """
    # Initialize services with caching
    app_factory = initialize_services()

    # Download NLTK data once
    download_nltk_data()

    # Initialize chat history from file if not in session state
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = load_chat_history()

    # Get services from AppFactory
    rag_pipeline = app_factory.get_rag_pipeline()
    image_database = app_factory.get_image_database()
    config = app_factory.get_config()

    # Debug session state
    debug_session_state()

    # DISABLED: Auto-reload documents to prevent infinite loop
    # Only set flag to prevent future calls
    if not st.session_state.get("auto_reloaded", False):
        st.session_state.auto_reloaded = True

    # MANUAL: Load documents only once when needed
    if not st.session_state.get("processed_files") and os.path.exists(UPLOADS_DIR):
        files = [f for f in os.listdir(UPLOADS_DIR) if os.path.isfile(os.path.join(UPLOADS_DIR, f))]
        if files and not st.session_state.get("_manual_loaded", False):
            st.session_state._manual_loaded = True
            # Load documents manually without auto-reload
            if "processed_files" not in st.session_state:
                st.session_state.processed_files = []

            for file_name in files:
                if file_name not in st.session_state.processed_files:
                    st.session_state.processed_files.append(file_name)

    # Display files in sidebar (no logging needed)
    if st.session_state.get("processed_files"):
        pass  # Files will be displayed in sidebar

    # Sidebar
    with st.sidebar:
        st.title("âš™ï¸ CÃ i Ä‘áº·t")

        # Show uploaded files list with scrollable container
        if st.session_state.get("processed_files"):
            st.subheader("ğŸ“„ Files")

            # Add search/filter for files
            search_term = st.text_input(
                "ğŸ” TÃ¬m file...", placeholder="Nháº­p tÃªn file", key="file_search"
            )

            # Filter files based on search
            files_to_show = st.session_state.processed_files
            if search_term:
                files_to_show = [f for f in files_to_show if search_term.lower() in f.lower()]

            # Show file count
            st.caption(f"ğŸ“Š {len(files_to_show)}/{len(st.session_state.processed_files)} files")

            # Scrollable container for files
            with st.container():
                # Limit display to first 10 files, show more button if needed
                max_display = 10
                files_display = files_to_show[:max_display]

                for i, file_name in enumerate(files_display):
                    # Truncate long filenames
                    display_name = file_name
                    if len(file_name) > 25:
                        display_name = file_name[:20] + "..." + file_name[-4:]

                    # Add file type icon
                    file_ext = file_name.split(".")[-1].lower() if "." in file_name else "txt"
                    icon_map = {
                        "pdf": "ğŸ“„",
                        "docx": "ğŸ“",
                        "xlsx": "ğŸ“Š",
                        "pptx": "ğŸ“ˆ",
                        "txt": "ğŸ“ƒ",
                        "md": "ğŸ“‹",
                        "json": "ğŸ“‹",
                        "csv": "ğŸ“Š",
                    }
                    icon = icon_map.get(file_ext, "ğŸ“„")

                    st.write(f"{icon} {display_name}")

                # Show "more files" button if needed
                if len(files_to_show) > max_display:
                    if st.button(
                        f"ğŸ“‹ Xem thÃªm {len(files_to_show) - max_display} files...",
                        key="show_more_files",
                    ):
                        st.session_state.show_all_files = True
                        st.rerun()

                # Show all files if requested
                if st.session_state.get("show_all_files", False):
                    st.write("---")
                    for file_name in files_to_show[max_display:]:
                        display_name = file_name
                        if len(file_name) > 25:
                            display_name = file_name[:20] + "..." + file_name[-4:]
                        file_ext = file_name.split(".")[-1].lower() if "." in file_name else "txt"
                        icon = icon_map.get(file_ext, "ğŸ“„")
                        st.write(f"{icon} {display_name}")

                    if st.button("ğŸ“‹ Thu gá»n", key="collapse_files"):
                        st.session_state.show_all_files = False
                        st.rerun()
        else:
            st.write("ğŸ“„ No files uploaded")

        # Add new file button
        if st.button("ğŸ“ ThÃªm file má»›i", type="primary"):
            st.session_state.force_show_upload = True
            st.rerun()

        # Delete all files button
        if st.session_state.get("processed_files"):
            if st.button(
                "ğŸ—‘ï¸ XÃ³a háº¿t file",
                type="secondary",
                help="XÃ³a táº¥t cáº£ files khá»i database vÃ  uploads",
            ):
                # Clear files from database
                if rag_pipeline:
                    rag_pipeline.clear_documents()

                # Clear files from uploads folder
                import shutil

                if os.path.exists(UPLOADS_DIR):
                    shutil.rmtree(UPLOADS_DIR)
                    os.makedirs(UPLOADS_DIR)

                # Clear session state
                st.session_state.processed_files = []
                st.session_state.auto_reloaded = False

                st.success("âœ… ÄÃ£ xÃ³a háº¿t files!")
                st.rerun()

        # Clear chat button
        if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat", type="secondary"):
            st.session_state.chat_history = []
            save_chat_history([])
            st.rerun()

        # (Removed) Buttons for Weaviate chat operations per user request

    # Main content area
    st.title("ğŸ¤– Hybrid RAG Chatbot")
    st.markdown("**Hybrid RAG Pipeline vá»›i Haystack Core + LangChain Fallback**")

    # Show files summary when files exist
    if st.session_state.get("processed_files"):
        st.subheader("ğŸ“š Documents Ready")
        st.success(
            f"âœ… {len(st.session_state.processed_files)} documents loaded and ready for chat!"
        )

    # Chat interface
    st.subheader("ğŸ’¬ Chat")

    # Display chat history
    if st.session_state.get("chat_history"):
        for message in st.session_state.chat_history:
            with st.chat_message("user"):
                st.write(message["question"])
            with st.chat_message("assistant"):
                st.write(message["answer"])
                # Persisted images per message
                imgs = message.get("images") or []
                if imgs:
                    st.info("ğŸ–¼ï¸ **áº¢nh liÃªn quan trong tÃ i liá»‡u:**")
                    for i, img in enumerate(imgs[:1]):  # show at most 1 image per message
                        try:
                            st.image(
                                img.get("path"),
                                caption=(img.get("context", "")[:100] + "..."),
                            )
                        except Exception:
                            pass
                if message.get("sources"):
                    with st.expander("ğŸ“š Nguá»“n tham kháº£o"):
                        for source in message["sources"]:
                            st.write(f"ğŸ“„ {source}")

    # Chat input
    prompt = st.chat_input("Ask anything...")

    # Process chat input
    if prompt:
        if rag_pipeline is not None:
            process_chat_input_old(prompt, rag_pipeline, image_database)

    # File upload section - Only show when no files or force show
    has_files = bool(st.session_state.get("processed_files"))
    force_show = st.session_state.get("force_show_upload", False)

    if not has_files or force_show:
        st.subheader("ğŸ“ Upload Documents")
        st.info(
            "ğŸ’¡ Upload documents Ä‘á»ƒ chat vá»›i AI. Há»— trá»£: PDF, DOCX, TXT, MD, XLSX, CSV, HTML, JSON"
        )

        uploaded_files = st.file_uploader(
            "Choose files",
            type=config.processing.supported_formats,
            accept_multiple_files=True,
            help="PDF, DOCX, TXT, MD, MARKDOWN, XLSX, XLS, PPTX, HTML, HTM, JSON, CSV",
        )

        if uploaded_files:
            # Process uploaded files
            process_uploaded_files_old(uploaded_files, rag_pipeline, image_database)

            # Hide upload area after processing
            st.session_state.force_show_upload = False
            st.rerun()


def fix_double_extension(filename: str) -> str:
    """
    ğŸ”§ Fix common double extension issues

    ğŸ”„ Luá»“ng Ä‘i:
    1. Define extensions mapping
    2. Check filename cÃ³ double extension
    3. Replace vá»›i single extension
    4. Return fixed filename

    ğŸ“Š Purpose: Xá»­ lÃ½ lá»—i upload file cÃ³ double extension
    ğŸ” Common cases: .pdf.pdf, .docx.docx, .xlsx.xlsx, .md.md
    """
    # More comprehensive extension mapping
    extensions_map = {
        ".pdf.pdf": ".pdf",
        ".docx.docx": ".docx",
        ".xlsx.xlsx": ".xlsx",
        ".md.md": ".md",
        ".txt.txt": ".txt",
        ".csv.csv": ".csv",
        ".json.json": ".json",
        ".html.html": ".html",
        ".htm.htm": ".htm",
        ".pptx.pptx": ".pptx",
        ".xls.xls": ".xls",
    }

    # Check for double extensions
    for old_ext, new_ext in extensions_map.items():
        if filename.endswith(old_ext):
            return filename.replace(old_ext, new_ext)

    # Also check for any double extension pattern
    import re

    # Pattern to match double extensions like .ext.ext
    double_ext_pattern = r"\.([^.]+)\.\1$"
    match = re.search(double_ext_pattern, filename)
    if match:
        ext = match.group(1)
        return filename.replace(f".{ext}.{ext}", f".{ext}")

    return filename


def save_uploaded_file(uploaded_file, uploads_dir: str) -> str:
    """
    ğŸ’¾ Save uploaded file to disk vá»›i proper error handling

    ğŸ”„ Luá»“ng Ä‘i:
    1. Create uploads directory náº¿u khÃ´ng tá»“n táº¡i
    2. Fix double extensions
    3. Generate file path
    4. Write file content
    5. Return file path

    ğŸ“Š Error Handling: Create directory náº¿u khÃ´ng tá»“n táº¡i
    ğŸ”§ File Processing: Fix double extensions trÆ°á»›c khi save
    """
    if not os.path.exists(uploads_dir):
        os.makedirs(uploads_dir)

    # Fix double extensions
    file_name = fix_double_extension(uploaded_file.name)
    file_path = os.path.join(uploads_dir, file_name)

    with open(file_path, "wb") as f:
        f.write(uploaded_file.getvalue())

    return file_path


def process_uploaded_files_old(uploaded_files, rag_pipeline, image_database) -> None:
    """Process uploaded files with enhanced error handling and status display"""
    if not uploaded_files:
        return

    # Initialize processed_files if not exists
    if "processed_files" not in st.session_state:
        st.session_state.processed_files = []

    processed_count = 0
    total_files = len(uploaded_files)

    # Create a status container
    status_container = st.empty()

    for i, uploaded_file in enumerate(uploaded_files):
        try:
            # Show simple status with fixed filename
            fixed_filename = fix_double_extension(uploaded_file.name)
            status_container.info(f"ğŸ”„ Äang xá»­ lÃ½: {fixed_filename} ({i + 1}/{total_files})")

            # Save file
            file_path = save_uploaded_file(uploaded_file, UPLOADS_DIR)

            # Process with document service
            documents = DocumentService().convert_file(file_path)

            if documents and rag_pipeline:
                # Add to RAG pipeline
                rag_pipeline.add_documents(documents)

                # Extract images
                # Use the fixed filename (the one actually saved) so source matching works
                images = image_database.extract_images_from_any_file(
                    file_path, fix_double_extension(uploaded_file.name)
                )

                # Update session state with fixed filename
                fixed_filename = fix_double_extension(uploaded_file.name)
                if fixed_filename not in st.session_state.processed_files:
                    st.session_state.processed_files.append(fixed_filename)

                processed_count += 1

                # Show success status
                status_container.success(f"âœ… ÄÃ£ xá»­ lÃ½: {fixed_filename}")

                # Show image extraction status if images found
                if images:
                    status_container.info(f"ğŸ–¼ï¸ ÄÃ£ trÃ­ch xuáº¥t {len(images)} áº£nh tá»« {fixed_filename}")
                else:
                    status_container.warning(f"âš ï¸ KhÃ´ng thá»ƒ xá»­ lÃ½: {fixed_filename}")

        except Exception as e:
            logger.error(f"âŒ Error processing {fixed_filename}: {e}")
            status_container.error(f"âŒ Lá»—i xá»­ lÃ½: {fixed_filename}")

    # Final success message
    if processed_count > 0:
        status_container.success(f"ğŸš€ ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng {processed_count} file!")
        st.rerun()
    else:
        status_container.error("âŒ KhÃ´ng cÃ³ file nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")


def process_chat_input_old(prompt, rag_pipeline, image_database):
    """
    ğŸ’¬ Process chat input vá»›i Hybrid RAG

    ğŸ”„ Luá»“ng Ä‘i:
    1. Display user message
    2. Query RAG pipeline
    3. Display AI answer
    4. Find relevant images:
       - Extract source files tá»« documents
       - Search images by source file
       - Fallback to query-based search
    5. Display images vá»›i download buttons
    6. Show sources
    7. Save to chat history

    ğŸ¯ Hybrid Pipeline:
    - Primary: Haystack Core (fast, feature-rich)
    - Fallback: LangChain (reliable, simple)
    - Auto-switch: Exception-based fallback

    ğŸ–¼ï¸ Image Display Logic:
    - Priority 1: Images tá»« source files
    - Priority 2: Query-based image search
    - Priority 3: Images tá»« previous question
    - Limit: 1 image Ä‘á»ƒ trÃ¡nh clutter

    ğŸ’¾ Persistence:
    - Save chat history sau má»—i interaction
    - Store displayed images cho persistence
    - UTF-8 encoding cho Vietnamese text
    """
    # Hiá»ƒn thá»‹ cÃ¢u há»i cá»§a user trong chat
    st.chat_message("user").write(prompt)

    # Láº¥y cÃ¢u tráº£ lá»i tá»« AI sá»­ dá»¥ng Hybrid RAG
    with st.chat_message("assistant"):
        try:
            # Sá»­ dá»¥ng RAG pipeline Ä‘á»ƒ tÃ¬m cÃ¢u tráº£ lá»i
            result = rag_pipeline.query(prompt)
            if not isinstance(result, dict):
                result = {"answer": str(result), "sources": []}

            # Display answer (parse JSON to render tables in Markdown when available)
            displayed_answer = str(result.get("answer", ""))
            parsed = None
            try:
                parsed = json.loads(displayed_answer)
            except Exception:
                try:
                    start = displayed_answer.find("{")
                    end = displayed_answer.rfind("}")
                    if start != -1 and end != -1 and end > start:
                        parsed = json.loads(displayed_answer[start : end + 1])
                except Exception:
                    parsed = None

            if isinstance(parsed, dict):
                ans = parsed.get("answer") or parsed.get("short_answer")
                det = parsed.get("details")
                tables = parsed.get("tables") or []
                if ans:
                    st.markdown(ans)
                if det:
                    st.markdown(det)
                for tbl in tables:
                    if isinstance(tbl, str) and "|" in tbl:
                        st.markdown(tbl)

                # Merge JSON sources with pipeline sources for display
                json_sources = []
                for s in parsed.get("sources", []) or []:
                    if isinstance(s, dict):
                        label = s.get("file") or s.get("source") or "Unknown"
                    else:
                        label = str(s)
                    if label:
                        json_sources.append(label)
                result["sources"] = list({*(result.get("sources", []) or []), *json_sources})

                # Prepare concise answer for history
                displayed_answer = (ans or "").strip()
                if det:
                    displayed_answer += "\n\n" + det.strip()
            else:
                st.markdown(displayed_answer, unsafe_allow_html=True)

            # Only show images when the user explicitly asks about images
            image_query = any(
                k in (prompt or "").lower()
                for k in ["hÃ¬nh", "áº£nh", "hÃ¬nh áº£nh", "image", "picture", "photo"]
            )
            persisted_imgs = []
            if image_query and ("KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin" not in result.get("answer", "")):
                # Collect source files from returned documents
                source_files = set()
                docs = result.get("documents", []) if isinstance(result, dict) else []
                for doc in docs:
                    meta = getattr(doc, "meta", None) or getattr(doc, "metadata", None) or {}
                    if isinstance(meta, dict):
                        src = meta.get("source", "")
                        if src:
                            try:
                                import os as _os

                                src = _os.path.basename(src)
                            except Exception:
                                pass
                            source_files.add(src)

                # Fetch images strictly by matched source files
                relevant_images = []
                for source_file in source_files:
                    relevant_images.extend(image_database.get_images_by_source(source_file))

                # If nothing matched by source, try query-based search (still only for image queries)
                if not relevant_images:
                    relevant_images = image_database.find_relevant_images(prompt, max_results=3)

                # Show at most one image if any matched
                if relevant_images:
                    unique_paths = set()
                    display_images = []
                    for img in relevant_images:
                        if img.get("path") and img["path"] not in unique_paths:
                            display_images.append(img)
                            unique_paths.add(img["path"])
                        if len(display_images) >= 1:
                            break

                    if display_images:
                        st.info("ğŸ–¼ï¸ **áº¢nh liÃªn quan trong tÃ i liá»‡u:**")
                        for i, img in enumerate(display_images):
                            st.image(
                                img["path"],
                                caption=img.get("context", "")[:100] + "...",
                            )
                            with open(img["path"], "rb") as fh:
                                st.download_button(
                                    "ğŸ“¥ Táº£i áº£nh",
                                    fh.read(),
                                    file_name=img.get("filename", "image.png"),
                                    mime="image/png",
                                    key=f"download_img_{i}_{img.get('filename', 'image')}",
                                )
                        # Persist images with this message
                        persisted_imgs = [
                            {
                                "path": im.get("path"),
                                "filename": im.get("filename"),
                                "context": im.get("context", ""),
                            }
                            for im in display_images
                        ]

            # Always show sources and save history (regardless of found/not found)
            sources = result.get("sources", [])
            if sources:
                with st.expander("ğŸ“š Nguá»“n tham kháº£o"):
                    unique_sources = []
                    for source in sources:
                        if source not in unique_sources:
                            unique_sources.append(source)
                    for i, source in enumerate(unique_sources):
                        st.write(f"**ğŸ“„ {source}**")

            if "chat_history" not in st.session_state:
                st.session_state.chat_history = []
            st.session_state.chat_history.append(
                {
                    "question": prompt,
                    "answer": displayed_answer or result.get("answer", ""),
                    "sources": sources,
                    "images": persisted_imgs,
                    "timestamp": datetime.now().isoformat(),
                }
            )
            # Save chat history after each interaction
            save_chat_history(st.session_state.chat_history)

            # Persist chat to Weaviate if available
            try:
                if hasattr(rag_pipeline, "document_store") and hasattr(
                    rag_pipeline.document_store, "write_chat_interaction"
                ):
                    rag_pipeline.document_store.write_chat_interaction(
                        question=prompt,
                        answer=displayed_answer or result.get("answer", ""),
                        sources=sources,
                        timestamp=datetime.now().isoformat(),
                    )
            except Exception as e:
                logger.warning(f"Could not persist chat to Weaviate: {e}")
        except Exception as e:
            st.error(f"âŒ Lá»—i: {str(e)}")


def extract_source_files(source_documents):
    """
    ğŸ“„ Extract source files tá»« documents

    ğŸ”„ Luá»“ng Ä‘i:
    1. Iterate qua source documents
    2. Extract source file tá»« metadata
    3. Fallback to content parsing náº¿u khÃ´ng cÃ³ metadata
    4. Use regex Ä‘á»ƒ extract filename tá»« path
    5. Return unique list of source files

    ğŸ“Š Purpose: TÃ¬m source files Ä‘á»ƒ image search
    ğŸ” Fallback: Parse content náº¿u metadata khÃ´ng cÃ³
    ğŸ¯ Result: List unique source files
    """
    source_files = set()

    for doc in source_documents:
        if hasattr(doc, "metadata"):
            source_file = doc.metadata.get("source", "")
        else:
            # Try to extract from document content
            content = str(doc)
            import re

            file_match = re.search(r"uploads[/\\]([^/\s]+)", content)
            if file_match:
                source_file = file_match.group(1)
            else:
                source_file = ""

        if source_file:
            source_files.add(source_file)

    return list(source_files)


if __name__ == "__main__":
    main()
